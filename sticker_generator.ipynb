{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a86891c-26c7-45b2-86a5-67e94caf1188",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd41074-bb0b-4429-9e6c-005181ba0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install segment-anything\n",
    "#pip install yolo5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c011b3-3ed8-4cd6-9928-6b8c3aec4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "from yolov5 import YOLOv5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744d2cd-f521-458f-a8e5-754a03b10515",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxillary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24049ae7-f4eb-4d3d-be90-3086726a3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to show box / mask taken and modified from facebookresearch/segment-anything repository\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "    \n",
    "\n",
    "def show_mask(mask, image, ax, random_color = True, return_sticker = False):\n",
    "    h, w = mask.shape[-2:]\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    \n",
    "    if return_sticker == False:\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "            color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "        mask_image_pil = Image.fromarray((mask_image * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "        return np.asarray(Image.alpha_composite(annotated_frame_pil, mask_image_pil))\n",
    "    \n",
    "    else:\n",
    "        mask_image = mask.reshape(h, w, 1) * annotated_frame_pil\n",
    "        mask_image_pil = Image.fromarray((mask_image).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "        return mask_image_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b2f07-0aed-408e-ae04-48e31c01b761",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "59b2f5fe-a580-4e6c-8cba-3dc2e4f2d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "local_image_path = 'assets/bird.jpg'\n",
    "image = cv2.imread(local_image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38c6efca-878a-4b09-a059-832725d54928",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c2d30-5163-4c0a-b947-5a5f8cda4cc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load YOLOv5 and SA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af77f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2023-4-16 Python-3.10.9 torch-2.0.0+cpu CPU\n",
      "\n",
      "D:\\Github Repositories\\Virtual Environments\\super_segmentation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 270 layers, 7235389 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Install the YOLOv5 model\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# set model params\n",
    "model_path = \"yolov5/weights/yolov5s.pt\" # it automatically downloads yolov5s model to given path\n",
    "\n",
    "# initialize the model\n",
    "yolov5 = YOLOv5(model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65192706-cac5-44ec-bba6-d23dfab6e714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-04-18 00:00:06--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... failed: No such host is known. .\n",
      "wget: unable to resolve host address 'dl.fbaipublicfiles.com'\n"
     ]
    }
   ],
   "source": [
    "# Download the pretrained weights for the SAM\n",
    "\n",
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2b85b7-f692-4173-be6a-d54be6211224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VIT-H is the default model\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "\n",
    "sam = build_sam(checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de371875-933a-4375-8bb7-96db2ceda74c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Output from YOLOv5 and SA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c15ecb7c-5a33-4e3d-ad29-b16f149c1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference\n",
    "results = yolov5.predict(image)\n",
    "box = []\n",
    "for detection in results.pred:\n",
    "    for i in range(len(detection.detach().cpu().numpy())):\n",
    "        box.append((detection.detach().cpu().numpy())[i][:4])\n",
    "        box[i] = np.array(box[i])\n",
    "box = np.asarray(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e4eafe97-9fcf-46d2-b74b-574ffd07a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "for boxes in box:\n",
    "    show_box(boxes, plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3357081b-15e4-4d13-96fb-535db4f1bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2a8f0e0c-2bb2-4241-9446-8510f3e89d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if box.shape[0] > 1:\n",
    "    boxes = torch.Tensor(box, device = sam_predictor.device)\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes, image.shape[:2])\n",
    "    masks, _, _ = sam_predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    masks, _, _ = sam_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box = box,\n",
    "        multimask_output=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a6d4f019-ce48-4fd3-9c9f-5a6ecbc04758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "for mask in masks:\n",
    "    segmented_mask = show_mask(mask.cpu().numpy(), image, plt.gca(), random_color = True, return_sticker = False)\n",
    "for boxes in box:\n",
    "    show_box(boxes, plt.gca())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2e1a10b9-5f12-482e-85c6-92db926aa862",
   "metadata": {},
   "outputs": [],
   "source": [
    "sticker = []\n",
    "for mask in masks:\n",
    "    sticker.append(show_mask(mask.cpu().numpy(), image, plt.gca(), return_sticker = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2fef900c-203d-440d-a3f7-98be7386aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sticker[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd6ec3b1-d142-49cf-8ed0-3974a5094a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sticker[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8dd65aa6-7646-46d6-b8b2-ba0a57f0bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output in results/\n",
    "\n",
    "directory = \"results/\" \n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "file_heading = 'sticker_'\n",
    "file_sub_heading = os.path.splitext(os.path.basename(local_image_path))[0]\n",
    "file_num = []\n",
    "file_ending = '.png'\n",
    "\n",
    "for i in range(len(sticker)):\n",
    "    file_num.append('_{}'.format(i + 1))\n",
    "    file_name = file_heading + file_sub_heading + file_num[i] + file_ending\n",
    "    if not os.path.exists(os.path.join(directory, file_name)):\n",
    "        sticker[i].save(os.path.join(directory, file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_segmentation",
   "language": "python",
   "name": "super_segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
